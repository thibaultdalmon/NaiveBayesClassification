{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis in textual movie reviews\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading dataset\")\n",
    "\n",
    "from glob import glob\n",
    "filenames_neg = sorted(glob(op.join('data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Comme toutes les critiques recueillies n'étaient pas directement associées à une note, celles-ci ont parfois été extraites du texte de la critique en utilisant des règles syntaxiques prédéfinies. Ces indications ont alors été supprimées des textes.\n",
    "De manière générale, les avis comportant une note supérieure à la moyenne sont positifs, les autres négatifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count_words function\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    vocabulary = {}\n",
    "    table = str.maketrans({key:\" \" for key in string.punctuation})\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    for text in texts:\n",
    "        word_list = text.translate(table).lower().split(\" \")\n",
    "        for word in word_list:\n",
    "            if word not in words:\n",
    "                words.add(word)\n",
    "                vocabulary[word] = j\n",
    "                j += 1\n",
    "\n",
    "    n_features = len(words)\n",
    "    counts = np.zeros((len(texts), n_features))\n",
    "\n",
    "    for text in texts:\n",
    "        word_list = text.translate(table).lower().split(\" \")\n",
    "        for word in word_list:\n",
    "            if word in words:\n",
    "                counts[i][vocabulary[word]] += 1\n",
    "        i += 1\n",
    "    \n",
    "    print(\"nombre de mots: \"+str(n_features))\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.prior = np.zeros((2))\n",
    "        self.condprob = None\n",
    "        self.scores = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.condprob = np.zeros((2, X.shape[1]))\n",
    "        for c in [0,1]:\n",
    "            self.prior[c] = X[y==c].shape[0] / X.shape[0]\n",
    "            self.condprob[c,:] = ((np.sum(X[y==c], axis=0) +1)\n",
    "                / np.sum(np.sum(X[y==c], axis=1)+1))\n",
    "        return self.vocabulary, self.prior, self.condprob\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.scores = np.zeros((X.shape[0], self.prior.shape[0]))\n",
    "        self.scores += np.log(self.prior)\n",
    "        tmp = np.zeros((X.shape[0], X.shape[1], 2))\n",
    "        for c in [0,1]:\n",
    "            tmp[:,:,c] = np.multiply(X, self.condprob[c,:])\n",
    "        tmp[tmp==0] = 1\n",
    "        self.scores += np.sum(np.log(tmp), axis=1)\n",
    "        return np.argmax(self.scores, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de mots: 44303\n",
      "0.815\n"
     ]
    }
   ],
   "source": [
    "# Count words in text\n",
    "vocabulary, X = count_words(texts)\n",
    "\n",
    "# Try to fit, predict and score\n",
    "nb = NB(vocabulary)\n",
    "nb.fit(X[::2], y[::2])\n",
    "print (nb.score(X[1::2], y[1::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n",
      "0.8\n",
      "0.7975\n",
      "0.7825\n",
      "0.735\n",
      "score moyen cross-validation: 0.779\n"
     ]
    }
   ],
   "source": [
    "# Try to fit, predict and score using cross-validation 5-folds\n",
    "X_dict = {}\n",
    "y_dict = {}\n",
    "for i in range(5):\n",
    "    X_dict[f\"X_{i}\"] = X[i::5]\n",
    "    y_dict[f\"y_{i}\"] = y[i::5]\n",
    "\n",
    "score = 0\n",
    "for i in range(5):\n",
    "    nb = NB(vocabulary)\n",
    "    X_val = X_dict[f\"X_{i}\"]\n",
    "    y_val = y_dict[f\"y_{i}\"]\n",
    "    first = True\n",
    "    for j in range(5):\n",
    "        if (j!=i):\n",
    "            if bool:\n",
    "                first = False\n",
    "                X_train = X_dict[f\"X_{j}\"]\n",
    "                y_train = y_dict[f\"y_{j}\"]\n",
    "            else:\n",
    "                X_train = np.concatenate((X_train, X_dict[f\"X_{j}\"]), axis=0)\n",
    "                y_train = np.concatenate((y_train, y_dict[f\"y_{j}\"]), axis=0)\n",
    "    nb.fit(X_train, y_train)\n",
    "    score_tmp = nb.score(X_val, y_val)\n",
    "    print(score_tmp)\n",
    "    score += score_tmp\n",
    "print(\"score moyen cross-validation: \"+str(score/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count_words version 2\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_V2(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    vocabulary = {}\n",
    "    table = str.maketrans({key:\" \" for key in string.punctuation})\n",
    "    i = 0\n",
    "    j = 0\n",
    "    english_words = open(op.join('data', 'english.stop')).read()\n",
    "\n",
    "    for text in texts:\n",
    "        word_list = text.translate(table).lower().split(\" \")\n",
    "        for word in word_list:\n",
    "            if (word not in words) and (word not in english_words):\n",
    "                words.add(word)\n",
    "                vocabulary[word] = j\n",
    "                j += 1\n",
    "\n",
    "    n_features = len(words)\n",
    "    counts = np.zeros((len(texts), n_features))\n",
    "\n",
    "    for text in texts:\n",
    "        word_list = text.translate(table).lower().split(\" \")\n",
    "        for word in word_list:\n",
    "            if word in words:\n",
    "                counts[i][vocabulary[word]] += 1\n",
    "        i += 1\n",
    "    print(\"nombre de mots: \"+str(n_features))\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de mots: 42843\n",
      "0.808\n"
     ]
    }
   ],
   "source": [
    "# Count words in text\n",
    "vocabulary, X = count_words_V2(texts)\n",
    "\n",
    "# Try to fit, predict and score\n",
    "nb = NB(vocabulary)\n",
    "nb.fit(X[::2], y[::2])\n",
    "print (nb.score(X[1::2], y[1::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.79\n",
      "0.765\n",
      "0.77\n",
      "0.72\n",
      "score moyen cross-validation: 0.759\n"
     ]
    }
   ],
   "source": [
    "# Try to fit, predict and score using cross-validation 5-folds\n",
    "X_dict = {}\n",
    "y_dict = {}\n",
    "for i in range(5):\n",
    "    X_dict[f\"X_{i}\"] = X[i::5]\n",
    "    y_dict[f\"y_{i}\"] = y[i::5]\n",
    "\n",
    "score = 0\n",
    "for i in range(5):\n",
    "    nb = NB(vocabulary)\n",
    "    X_val = X_dict[f\"X_{i}\"]\n",
    "    y_val = y_dict[f\"y_{i}\"]\n",
    "    first = True\n",
    "    for j in range(5):\n",
    "        if (j!=i):\n",
    "            if bool:\n",
    "                first = False\n",
    "                X_train = X_dict[f\"X_{j}\"]\n",
    "                y_train = y_dict[f\"y_{j}\"]\n",
    "            else:\n",
    "                X_train = np.concatenate((X_train, X_dict[f\"X_{j}\"]), axis=0)\n",
    "                y_train = np.concatenate((y_train, y_dict[f\"y_{j}\"]), axis=0)\n",
    "    nb.fit(X_train, y_train)\n",
    "    score_tmp = nb.score(X_val, y_val)\n",
    "    print(score_tmp)\n",
    "    score += score_tmp\n",
    "print(\"score moyen cross-validation: \"+str(score/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 :** Il semble que le retrait des \"stop words\" dégrade légèrement les performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn use\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with sklearn\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.813"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = texts[::2], y[::2]\n",
    "X_test, y_test = texts[1::2], y[1::2]\n",
    "countVec = CountVectorizer()\n",
    "clf = MultinomialNB()\n",
    "cV_mNB = Pipeline([('countVec', countVec), ('multNB', clf)])\n",
    "cV_mNB.set_params().fit(X_train, y_train)\n",
    "cV_mNB.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.606"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = texts[::2], y[::2]\n",
    "X_test, y_test = texts[1::2], y[1::2]\n",
    "countVec = CountVectorizer()\n",
    "clf = MultinomialNB()\n",
    "cV_mNB = Pipeline([('countVec', countVec), ('multNB', clf)])\n",
    "cV_mNB.set_params(countVec__analyzer = \"char\").fit(X_train, y_train)\n",
    "cV_mNB.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.605"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = texts[::2], y[::2]\n",
    "X_test, y_test = texts[1::2], y[1::2]\n",
    "countVec = CountVectorizer()\n",
    "clf = MultinomialNB()\n",
    "cV_mNB = Pipeline([('countVec', countVec), ('multNB', clf)])\n",
    "cV_mNB.set_params(countVec__analyzer = \"char_wb\").fit(X_train, y_train)\n",
    "cV_mNB.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/thibault/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/thibault/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk import SnowballStemmer, pos_tag, word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.831"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = texts[::2], y[::2]\n",
    "X_test, y_test = texts[1::2], y[1::2]\n",
    "countVec = CountVectorizer()\n",
    "clf = LogisticRegression()\n",
    "cV_mNB = Pipeline([('countVec', countVec), ('logReg', clf)])\n",
    "cV_mNB.set_params(logReg__random_state=42, logReg__solver='liblinear').fit(X_train, y_train)\n",
    "cV_mNB.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec LinearSVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = texts[::2], y[::2]\n",
    "X_test, y_test = texts[1::2], y[1::2]\n",
    "countVec = CountVectorizer()\n",
    "clf = LinearSVC()\n",
    "cV_mNB = Pipeline([('countVec', countVec), ('linSVC', clf)])\n",
    "cV_mNB.set_params(linSVC__random_state=42, linSVC__max_iter=10000).fit(X_train, y_train)\n",
    "cV_mNB.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "texts_stemmed = []\n",
    "for text in texts:\n",
    "    text_stemmed = \"\"\n",
    "    for word in text.split(\" \"):\n",
    "        text_stemmed += stemmer.stem(word) + \" \"\n",
    "    texts_stemmed.append(text_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = texts_stemmed[::2], y[::2]\n",
    "X_test, y_test = texts_stemmed[1::2], y[1::2]\n",
    "countVec = CountVectorizer()\n",
    "clf = LogisticRegression()\n",
    "cV_mNB = Pipeline([('countVec', countVec), ('logReg', clf)])\n",
    "cV_mNB.set_params(logReg__random_state=42, logReg__solver='liblinear').fit(X_train, y_train)\n",
    "cV_mNB.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec LinearSVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = texts_stemmed[::2], y[::2]\n",
    "X_test, y_test = texts_stemmed[1::2], y[1::2]\n",
    "countVec = CountVectorizer()\n",
    "clf = LinearSVC()\n",
    "cV_mNB = Pipeline([('countVec', countVec), ('linSVC', clf)])\n",
    "cV_mNB.set_params(linSVC__random_state=42, linSVC__max_iter=20000).fit(X_train, y_train)\n",
    "cV_mNB.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tag = []\n",
    "k = 0\n",
    "for text in texts_stemmed:\n",
    "    txt = []\n",
    "    sents = sent_tokenize(text)\n",
    "    bool = True\n",
    "    for sent in sents:\n",
    "        if bool:\n",
    "            txt = pos_tag(word_tokenize(sent))\n",
    "            bool = False\n",
    "        else:\n",
    "            txt += pos_tag(word_tokenize(sent))\n",
    "    texts_tag.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_filtered = [\"\" for text in texts_tag]\n",
    "target_words = ['NN', 'NNP', 'NNS', 'VB', 'VBD', 'VBG', 'VBN',\n",
    "                'VBP', 'VBZ', 'RB', 'RBR', 'RBS', 'WRB', 'JJ',\n",
    "                'JJR', 'JJS']\n",
    "for text in range(len(texts_tag)):\n",
    "    for words in texts_tag[text]:\n",
    "        if (words[1] in target_words):\n",
    "            text_filtered[text]+=words[0] + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = text_filtered[::2], y[::2]\n",
    "X_test, y_test = text_filtered[1::2], y[1::2]\n",
    "countVec = CountVectorizer()\n",
    "clf = LogisticRegression()\n",
    "cV_mNB = Pipeline([('countVec', countVec), ('logReg', clf)])\n",
    "cV_mNB.set_params(logReg__random_state=42, logReg__solver='liblinear').fit(X_train, y_train)\n",
    "cV_mNB.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec LinearSVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.815"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = text_filtered[::2], y[::2]\n",
    "X_test, y_test = text_filtered[1::2], y[1::2]\n",
    "countVec = CountVectorizer()\n",
    "clf = LinearSVC()\n",
    "cV_mNB = Pipeline([('countVec', countVec), ('linSVC', clf)])\n",
    "cV_mNB.set_params(linSVC__random_state=42, linSVC__max_iter=20000).fit(X_train, y_train)\n",
    "cV_mNB.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les meilleurs résultats sont obtenus lors de l'utilisation d'une méthode de filtrage des mots par type et en ne concervant que leur racine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
